{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94771,"databundleVersionId":13044405,"sourceType":"competition"},{"sourceId":12770695,"sourceType":"datasetVersion","datasetId":8073302}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Leak-Prevention (what the code guarantees)\n* **Chronology enforced**: train and train_labels are sorted by date_id before CV/training (no forward peeking).\n* **Shared features only**: Features are built from (train ∩ test) − {date_id,row_id,is_scored} to avoid train/test drift and accidental look-ahead.\n* **Shape parity at inference**: Each model saves its training feature order; at predict time we reindex to that exact order (missing → 0, extras dropped) to kill shape/categorical mismatches.\n* **Label hygiene**: Each target is trained only on its non-null rows; predictions never return NaNs.\n* **Fold-local preprocessing**: Any casting/cleanup happens inside each fold (no statistics fitted across time or using validation/test).\n\n# CV from three angles (aligned with the competition metric)\n1. OOF TimeSeriesSplit (clean, generalization-oriented)\n* Chronological TSS after sorting by date_id.\n* Per-fold, per-target training on non-null labels only.\n* Metric = daily Spearman across 424 targets → Sharpe (mean/std across days).\n* Labels use the official filler −999999 for metric only; logs show per-fold Sharpe and OOF Sharpe.\n\n2. Clean Public-90 hold-out (practical offline proxy)\n* Hold out the last ~90 days by date_id; train on the rest; evaluate with the same daily Spearman→Sharpe.\n* Serves as a safer proxy for future data (often lower than public LB; better indicator of robustness).\n\n3. Leaky Public-90 diagnostic (public LB comparator)\n* Compute a “leaky” score on the last ~90 days without strict separation to approximate the public LB.\n* he gap between Leaky Public-90 and Clean Public-90 flags public-LB inflation and weak future generalization.\n\n\n# リーク対策（コードで担保していること）\n* 時系列順の厳守： date_id で train と train_labels をソートしてから CV/学習（未来を覗かない）。\n* 共通特徴量のみ： 特徴量は (train ∩ test) − {date_id,row_id,is_scored} から作成（データドリフト＆先読みを防止）。\n* 推論時の形状固定： 学習時の特徴量順序を保存し、推論時は同じ順序に reindex（不足は0埋め・余分は削除）で不一致を根絶。\n* ラベルの衛生： 各ターゲットは 非欠損行のみで学習し、予測は NaN を返さない。\n* Fold 内完結の前処理： キャスト/クリーンアップは 各Foldの中で実施（時系列や検証/テストを跨ぐ情報漏れ無し）。\n\n\n# 3つのCV観点（評価指標に整合）\n# 1. OOF TimeSeriesSplit（クリーンで汎化志向）\n* date_id でソート後に時系列TSS。\n* Fold×Targetごとに非欠損のみで学習。\n* 指標＝日次のSpearman（424ターゲット横持ち）→ Sharpe（平均/標準偏差）。\n* メトリック用にラベルは −999999 で埋め、Fold別SharpeとOOF Sharpeを出力。\n\n# 2. Clean Public-90 ホールドアウト（実務向けのオフライン代理）\n* 末尾 約90日 を date_id でホールドアウト、残りで学習し同指標で評価。\n* 将来データの代理として安全（多くの場合 公開LBより低め、汎化の目安）。\n\n# 3. Leaky Public-90 診断（公開LBの近似）\n* 末尾約90日で“甘め”にスコアを算出し、公開LBに近い値を得る。\n* Leaky と Clean の 差が大きいほど、公開LBの見かけ上の高さと将来汎化の弱さを示唆。\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# ==============================================================================\n# CONFIGURATION\n# ==============================================================================\nTRAIN       = True    # True: train + (optional) CV + save; False: load + predict\nDO_CV       = True    # CV is only used when TRAIN=True\nCV_SPLITS   = 3\nDO_PUBLIC90 = True    # <- 追加: 末尾90日ホールドアウト（Public LB に近い検証）を実行するか\n\n# Force inference-only inside the scoring container (safety)\nIS_COMP_RUN = os.getenv('KAGGLE_IS_COMPETITION_RERUN') is not None\nif IS_COMP_RUN:\n    TRAIN  = False\n    DO_CV  = False\n    DO_PUBLIC90 = False  # スコアリング用コンテナでは必ず無効化\n\n# Detect Kaggle environment\nIS_KAGGLE = os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None\nif IS_KAGGLE:\n    import kaggle_evaluation.mitsui_inference_server\n    DATA_PATH        = '/kaggle/input/mitsui-commodity-prediction-challenge/'\n    MODEL_INPUT_DIR  = '/kaggle/input/model4-suke/'   # <- your model artifact dir\n    MODEL_OUTPUT_DIR = '/kaggle/working/model'\nelse:\n    DATA_PATH        = './'\n    MODEL_INPUT_DIR  = './model/'\n    MODEL_OUTPUT_DIR = './model'\nos.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n\nsns.set_theme(style=\"darkgrid\")\n\n# ==============================================================================\n# LOAD DATA\n# ==============================================================================\nprint(\"Loading data…\")\ntrain_df     = pl.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest_df      = pl.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ntrain_labels = pl.read_csv(os.path.join(DATA_PATH, 'train_labels.csv'))\nprint(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}, Labels shape: {train_labels.shape}\")\n\n# ------------------------------------------------------------------------------\n# Leak prevention #1: sort BOTH train and labels by date_id before CV/training\n# ------------------------------------------------------------------------------\nif 'date_id' in train_df.columns:\n    train_df     = train_df.sort('date_id')\nif 'date_id' in train_labels.columns:\n    train_labels = train_labels.sort('date_id')\n\n# ==============================================================================\n# FEATURE / TARGET DEFINITIONS\n# ==============================================================================\n# Leak prevention #2: build SHARED features from (train ∩ test) minus excludes\nEXCLUDE_COLS   = {'date_id', 'row_id', 'is_scored'}\nCOMMON_COLUMNS = [c for c in train_df.columns if c in set(test_df.columns)]\nFEATURE_BASE   = [c for c in COMMON_COLUMNS if c not in EXCLUDE_COLS]\n\nTARGETS = [f\"target_{i}\" for i in range(424)]\n\n# ==============================================================================\n# PREPROCESSING & FEATURE ENGINEERING\n# ==============================================================================\ndef preprocess_for_lgbm(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Convert object-type columns to numeric and pandas.Categorical to integer codes.\n    Keep dtypes stable between train and predict.\n    \"\"\"\n    df = df.copy()\n    for col in [\n        'US_Stock_GOLD_adj_open','US_Stock_GOLD_adj_high',\n        'US_Stock_GOLD_adj_low','US_Stock_GOLD_adj_close',\n        'US_Stock_GOLD_adj_volume'\n    ]:\n        if col in df and df[col].dtype == 'object':\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n    for cat in df.select_dtypes(['category']).columns:\n        df[cat] = df[cat].cat.codes\n    df = df.replace([np.inf, -np.inf], np.nan)\n    return df\n\ndef create_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Placeholder for feature engineering; returns input unchanged.\"\"\"\n    return df.clone()\n\n# ==============================================================================\n# CV METRICS (competition-like)\n# ==============================================================================\nSOLUTION_NULL_FILLER = -999999\n\ndef rankcorr_sharpe(preds_df: pd.DataFrame, truths_df: pd.DataFrame, filler: float = SOLUTION_NULL_FILLER) -> float:\n    \"\"\"\n    Competition-like metric:\n    - For each date (row), compute Spearman rank correlation across targets (columns)\n      between predictions and labels (ignoring filler).\n    - Return Sharpe ratio = mean / std of those daily correlations.\n    \"\"\"\n    P = preds_df.copy().fillna(0.0)\n    T = truths_df.copy().fillna(filler)\n\n    daily_scores = []\n    for p_row, t_row in zip(P.values, T.values):\n        mask = (t_row != filler) & np.isfinite(p_row)\n        if mask.sum() < 2:\n            daily_scores.append(0.0)\n            continue\n        p_rank = pd.Series(p_row[mask]).rank(method='average').to_numpy()\n        t_rank = pd.Series(t_row[mask]).rank(method='average').to_numpy()\n        if np.std(p_rank) == 0 or np.std(t_rank) == 0:\n            daily_scores.append(0.0)\n        else:\n            daily_scores.append(np.corrcoef(p_rank, t_rank)[0, 1])\n\n    arr = np.asarray(daily_scores, dtype=float)\n    std = arr.std(ddof=0)\n    return float(arr.mean() / std) if std > 0 else 0.0\n\n# 追加: Public-90 style 検証（末尾90日ホールドアウト）\ndef validate_public_90_style(train_pl: pl.DataFrame, labels_pl: pl.DataFrame) -> float:\n    \"\"\"\n    Train on head (N-90), validate on last 90 days (Kaggle public LB に近い構成)。\n    戻り値は Kaggle 互換 Sharpe。\n    \"\"\"\n    X_all_df = create_features(train_pl).to_pandas()\n    X_all    = X_all_df[FEATURE_BASE].copy()\n    y_all    = labels_pl.to_pandas()[TARGETS].copy()\n\n    N = len(X_all)\n    H = 90\n    if N <= H:\n        print(\"[Public-90] Not enough length; skipped.\")\n        return 0.0\n\n    tr_idx = np.arange(0, N - H)\n    va_idx = np.arange(N - H, N)\n\n    preds_va = pd.DataFrame(index=range(H), columns=TARGETS, dtype=float)\n    sol_va   = y_all.iloc[va_idx].reset_index(drop=True)\n\n    for idx, tgt in enumerate(TARGETS):\n        y = y_all[tgt]\n        mask_tr = (~y.isna()).values & (np.arange(N) < (N - H))\n        if mask_tr.sum() == 0:\n            preds_va[tgt] = 0.0\n            continue\n\n        Xtr = X_all.loc[mask_tr].copy()\n        ytr = y.loc[mask_tr]\n        Xva = X_all.iloc[va_idx].copy()\n\n        Xtr['target_name_encoded'] = idx\n        Xva['target_name_encoded'] = idx\n\n        Xtr = preprocess_for_lgbm(Xtr).reindex(columns=(FEATURE_BASE + ['target_name_encoded']), fill_value=0.0)\n        Xva = preprocess_for_lgbm(Xva).reindex(columns=(FEATURE_BASE + ['target_name_encoded']), fill_value=0.0)\n\n        m = LGBMRegressor(\n            n_estimators=100, learning_rate=0.05,\n            max_depth=-1, num_leaves=64,\n            subsample=0.8, colsample_bytree=0.8,\n            random_state=42, verbose=-1\n        )\n        m.fit(Xtr, ytr)\n        preds_va[tgt] = m.predict(Xva)\n\n    score = rankcorr_sharpe(preds_va, sol_va, filler=SOLUTION_NULL_FILLER)\n    return float(score)\n\n# ==============================================================================\n# MODEL STORAGE / METADATA\n# ==============================================================================\nmodels         = {}  # tgt -> LightGBM model\nMODEL_FEATURES = {}  # tgt -> list of feature names used at train time (order matters)\nMODELS_LOADED  = False\n\ndef _meta_path_for(tgt: str) -> str:\n    return os.path.join(MODEL_OUTPUT_DIR, f\"{tgt}_feat.pkl\")\n\ndef _load_model_and_meta(tgt: str, input_dir: str):\n    mpath = os.path.join(input_dir, f\"{tgt}_model.pkl\")\n    fpath = os.path.join(input_dir, f\"{tgt}_feat.pkl\")\n    model = joblib.load(mpath) if os.path.exists(mpath) else None\n    feats = None\n    if os.path.exists(fpath):\n        try:\n            feats = joblib.load(fpath)\n        except Exception:\n            feats = None\n    return model, feats\n\ndef _fallback_feature_order_for_model(model) -> list:\n    n = getattr(model, \"n_features_\", None)\n    if not isinstance(n, (int, np.integer)) or n < 1:\n        return (FEATURE_BASE + ['target_name_encoded'])\n    take = max(0, n - 1)\n    return (FEATURE_BASE[:take] + ['target_name_encoded'])\n\ndef _lazy_load_models():\n    global MODELS_LOADED, models, MODEL_FEATURES\n    if MODELS_LOADED:\n        return\n    loaded = 0\n    for tgt in TARGETS:\n        model, feats = _load_model_and_meta(tgt, MODEL_INPUT_DIR)\n        models[tgt] = model\n        if (model is not None) and (feats is None):\n            feats = _fallback_feature_order_for_model(model)\n        MODEL_FEATURES[tgt] = feats if feats is not None else (FEATURE_BASE + ['target_name_encoded'])\n        if models[tgt] is not None:\n            loaded += 1\n    MODELS_LOADED = True\n    print(f\"[Info] Lazy-loaded models: {loaded} / {len(TARGETS)}\")\n\n# ==============================================================================\n# TRAINING MODE\n# ==============================================================================\nif TRAIN:\n    print(\"\\n======== TRAINING MODE ========\")\n    X_all_df = create_features(train_df).to_pandas()\n    X_all    = X_all_df[FEATURE_BASE]\n    y_all    = train_labels.to_pandas()[TARGETS]\n\n    for idx, tgt in enumerate(TARGETS):\n        print(f\"Training {tgt} …\")\n        y = y_all[tgt]\n        mask = ~y.isna()\n        X_tr_base = X_all.loc[mask].copy()\n        y_tr      = y.loc[mask]\n        if len(y_tr) == 0:\n            print(f\"  Skip {tgt}: no non-null labels.\")\n            models[tgt] = None\n            MODEL_FEATURES[tgt] = FEATURE_BASE + ['target_name_encoded']\n            joblib.dump(MODEL_FEATURES[tgt], _meta_path_for(tgt))\n            continue\n\n        X_tr = X_tr_base.copy()\n        X_tr['target_name_encoded'] = idx\n        X_tr = preprocess_for_lgbm(X_tr)\n\n        feat_order = FEATURE_BASE + ['target_name_encoded']\n        X_tr = X_tr.reindex(columns=feat_order, fill_value=0.0)\n\n        m = LGBMRegressor(\n            n_estimators=100,\n            learning_rate=0.05,\n            max_depth=-1,\n            num_leaves=64,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            verbose=-1\n        )\n        m.fit(X_tr, y_tr)\n\n        models[tgt] = m\n        MODEL_FEATURES[tgt] = feat_order\n        joblib.dump(m, os.path.join(MODEL_OUTPUT_DIR, f\"{tgt}_model.pkl\"))\n        joblib.dump(feat_order, _meta_path_for(tgt))\n\n    print(f\"Saved models & metadata to: {MODEL_OUTPUT_DIR}\")\n\n    # --------------------------------------------------------------------\n    # OPTIONAL TimeSeriesSplit CV (RankCorr-Sharpe) with leak prevention\n    # --------------------------------------------------------------------\n    if DO_CV:\n        print(f\"\\nRunning TimeSeriesSplit CV with {CV_SPLITS} folds …\")\n        SOL_FILL = SOLUTION_NULL_FILLER\n\n        X_base  = X_all.copy()\n        y_truth = y_all.copy()\n        y_score = y_truth.fillna(SOL_FILL)\n\n        oof = pd.DataFrame(index=y_truth.index, columns=TARGETS, dtype=float)\n        tscv = TimeSeriesSplit(n_splits=CV_SPLITS)\n        fold_scores = []\n        for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_base)):\n            print(f\"  CV Fold {fold+1}/{CV_SPLITS}\")\n            X_tr_b, X_va_b = X_base.iloc[tr_idx], X_base.iloc[va_idx]\n            y_tr_all       = y_truth.iloc[tr_idx]\n\n            for idx, tgt in enumerate(TARGETS):\n                y_tr_tgt = y_tr_all[tgt]\n                mask_tr  = ~y_tr_tgt.isna()\n                if mask_tr.sum() == 0:\n                    continue\n\n                Xtr = X_tr_b.loc[mask_tr].copy()\n                ytr = y_tr_tgt.loc[mask_tr]\n                Xva = X_va_b.copy()\n\n                Xtr['target_name_encoded'] = idx\n                Xva['target_name_encoded'] = idx\n\n                Xtr = preprocess_for_lgbm(Xtr).reindex(columns=(FEATURE_BASE + ['target_name_encoded']), fill_value=0.0)\n                Xva = preprocess_for_lgbm(Xva).reindex(columns=(FEATURE_BASE + ['target_name_encoded']), fill_value=0.0)\n\n                m_cv = LGBMRegressor(\n                    n_estimators=80,\n                    learning_rate=0.05,\n                    max_depth=-1,\n                    num_leaves=64,\n                    subsample=0.8,\n                    colsample_bytree=0.8,\n                    random_state=42,\n                    verbose=-1\n                )\n                m_cv.fit(Xtr, ytr)\n                oof.loc[va_idx, tgt] = m_cv.predict(Xva)\n\n            fold_score = rankcorr_sharpe(\n                preds_df=oof.loc[va_idx, TARGETS],\n                truths_df=y_score.loc[va_idx, TARGETS],\n                filler=SOL_FILL\n            )\n            fold_scores.append(fold_score)\n            print(f\"   -> Fold {fold+1} Sharpe: {fold_score:.4f}\")\n\n        cv_score = rankcorr_sharpe(oof, y_score, SOL_FILL)\n        print(f\"  OOF RankCorr-Sharpe = {cv_score:.4f}\")\n        if len(fold_scores) > 0:\n            print(f\"  Mean(Fold Sharpe)   = {np.mean(fold_scores):.4f}\")\n    else:\n        print(\"CV disabled (DO_CV=False).\")\n\n    # --------------------------------------------------------------------\n    # 追加: Public-90 style validation（末尾90日ホールドアウト）\n    # --------------------------------------------------------------------\n    if DO_PUBLIC90:\n        print(\"\\nRunning Public-90 style hold-out (last ~90 days) …\")\n        pub90_score = validate_public_90_style(train_df, train_labels)\n        print(f\"  Public-90 Sharpe     = {pub90_score:.4f}\")\n\nelse:\n    print(\"\\n======== INFERENCE MODE ========\")\n    print(f\"Models will be lazy-loaded from: {MODEL_INPUT_DIR}\")\n\n# ==============================================================================\n# PREDICTION FUNCTION for Kaggle Server (leak-safe + shape-safe)\n# ==============================================================================\ndef predict(\n    test: pl.DataFrame,\n    label_lags_1_batch: pl.DataFrame,\n    label_lags_2_batch: pl.DataFrame,\n    label_lags_3_batch: pl.DataFrame,\n    label_lags_4_batch: pl.DataFrame,\n) -> pd.DataFrame:\n    \"\"\"\n    Leak-safe & shape-safe inference:\n      - Use only intersection-based features (FEATURE_BASE).\n      - Add per-target constant 'target_name_encoded'.\n      - Preprocess and reindex to the EXACT per-model feature order saved at training.\n      - Fill missing columns with 0 and drop extras to avoid shape mismatches.\n      - Never return NaNs (fallback to 0.0 if a model is missing).\n      - Return a Pandas DataFrame with columns TARGETS in the exact order.\n    \"\"\"\n    _lazy_load_models()\n\n    df_feat = create_features(test)\n    X_base  = df_feat.to_pandas()\n\n    base_selected = X_base.reindex(columns=FEATURE_BASE, fill_value=0.0)\n\n    n_rows = len(base_selected)\n    out = np.zeros((n_rows, len(TARGETS)), dtype=np.float64)\n\n    for idx, tgt in enumerate(TARGETS):\n        model = models.get(tgt)\n        if model is None:\n            out[:, idx] = 0.0\n            continue\n\n        feat_order = MODEL_FEATURES.get(tgt)\n        if feat_order is None:\n            feat_order = _fallback_feature_order_for_model(model)\n\n        X_tmp = base_selected.copy()\n        X_tmp['target_name_encoded'] = idx\n\n        X_tmp = preprocess_for_lgbm(X_tmp)\n        X_tmp = X_tmp.reindex(columns=feat_order, fill_value=0.0)\n\n        pred = model.predict(X_tmp, validate_features=False)\n        pred = np.asarray(pred, dtype=np.float64).reshape(-1)\n        if pred.shape[0] != n_rows:\n            if pred.shape[0] > n_rows:\n                pred = pred[:n_rows]\n            else:\n                pred = np.pad(pred, (0, n_rows - pred.shape[0]), constant_values=0.0)\n        out[:, idx] = pred\n\n    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n\n    predictions = pd.DataFrame(out, columns=TARGETS)\n    assert predictions.shape[1] == 424, f\"Expected 424 columns, got {predictions.shape[1]}\"\n    assert list(predictions.columns) == TARGETS, \"Column order/name mismatch\"\n    assert np.isfinite(predictions.to_numpy()).all(), \"Non-finite values in predictions\"\n    return predictions\n\n# ==============================================================================\n# ENTRYPOINT / LAUNCH SERVER\n# ==============================================================================\nif IS_KAGGLE:\n    server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n    if IS_COMP_RUN:\n        server.serve()\n    else:\n        server.run_local_gateway((DATA_PATH,))\nelse:\n    mock = pl.DataFrame()\n    submission = predict(test_df, mock, mock, mock, mock)\n    submission = submission[TARGETS]\n    submission.to_csv(\"submission_local.csv\", index=False)\n    print(\"Local submission saved → submission_local.csv\")\n","metadata":{"_uuid":"150bbbec-d717-465e-bf91-ec9fad10baca","_cell_guid":"d3785fd1-0685-4416-8669-f517dbd8ce7b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-17T03:22:12.422183Z","iopub.execute_input":"2025-08-17T03:22:12.422459Z","iopub.status.idle":"2025-08-17T05:28:01.105525Z","shell.execute_reply.started":"2025-08-17T03:22:12.422433Z","shell.execute_reply":"2025-08-17T05:28:01.104837Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Diagnostic: Leaky Public-90 (full learning → last 90 days evaluation) ===\ndef validate_public_90_style_leaky(train_pl: pl.DataFrame, labels_pl: pl.DataFrame) -> float:\n    X_all_df = create_features(train_pl).to_pandas()\n    X_all    = X_all_df[FEATURE_BASE].copy()\n    y_all    = labels_pl.to_pandas()[TARGETS].copy()\n\n    N, H = len(X_all), 90\n    if N <= H:\n        print(\"[Leaky Public-90] Not enough length; skipped.\")\n        return 0.0\n\n    va_idx = np.arange(N - H, N)\n    preds_va = pd.DataFrame(index=range(H), columns=TARGETS, dtype=float)\n    sol_va   = y_all.iloc[va_idx].reset_index(drop=True)\n\n    for idx, tgt in enumerate(TARGETS):\n        y   = y_all[tgt]\n        msk = ~y.isna()\n        if msk.sum() == 0:\n            preds_va[tgt] = 0.0\n            continue\n        Xtr = X_all.loc[msk].copy(); ytr = y.loc[msk]\n        Xva = X_all.iloc[va_idx].copy()\n        Xtr[\"target_name_encoded\"] = idx\n        Xva[\"target_name_encoded\"] = idx\n        Xtr = preprocess_for_lgbm(Xtr).reindex(columns=(FEATURE_BASE+[\"target_name_encoded\"]), fill_value=0.0)\n        Xva = preprocess_for_lgbm(Xva).reindex(columns=(FEATURE_BASE+[\"target_name_encoded\"]), fill_value=0.0)\n        m = LGBMRegressor(\n            n_estimators=100, learning_rate=0.05,\n            max_depth=-1, num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n            random_state=42, verbose=-1\n        )\n        m.fit(Xtr, ytr)\n        preds_va[tgt] = m.predict(Xva)\n\n    return rankcorr_sharpe(preds_va, sol_va, filler=SOLUTION_NULL_FILLER)\n\nif TRAIN and DO_PUBLIC90 and not IS_COMP_RUN:\n    print(\"\\n[Diag] Leaky Public-90 style (train=full → val=last 90) …\")\n    leaky_score = validate_public_90_style_leaky(train_df, train_labels)\n    print(f\"  Leaky Public-90 Sharpe = {leaky_score:.4f}  “(Reference value: tends to be closer to the public leaderboard score)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T05:57:29.145953Z","iopub.execute_input":"2025-08-17T05:57:29.146234Z","iopub.status.idle":"2025-08-17T06:36:57.561954Z","shell.execute_reply.started":"2025-08-17T05:57:29.146212Z","shell.execute_reply":"2025-08-17T06:36:57.561183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Zip LightGBM model artifacts in /kaggle/working/model into a single archive ---\n# Notes:\n# - Comments are in English (Kaggle requirement)\n# - It zips all *.pkl files under /kaggle/working/model (recursively)\n# - It prints progress every 50 files, verifies archive integrity, and shows a short preview\n\nimport os\nimport zipfile\nfrom pathlib import Path\n\n\n# ====== CONFIG ======\nSRC_DIR = Path(\"/kaggle/working/model\")        # directory containing your ~400 *.pkl files\nOUT_ZIP = Path(\"/kaggle/working/model_bundle.zip\")\nINCLUDE_EXTS = {\".pkl\"}                        # add more extensions if needed (e.g., '.txt', '.json')\n\n# ====== COLLECT FILES ======\n# Recursively find all files with allowed extensions\nfiles = [p for p in SRC_DIR.rglob(\"*\") if p.is_file() and p.suffix.lower() in INCLUDE_EXTS]\nfiles.sort()\n\nif not files:\n    raise SystemExit(f\"No files with extensions {INCLUDE_EXTS} found under {SRC_DIR}\")\n\nprint(f\"Found {len(files)} file(s) to zip from: {SRC_DIR}\")\n\n# ====== CREATE ZIP ======\n# Use deflated compression; compresslevel 6 is a good trade-off between size and speed\nwith zipfile.ZipFile(OUT_ZIP, mode=\"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as zf:\n    for i, fp in enumerate(files, 1):\n        # Keep relative paths inside the archive (no absolute paths)\n        arcname = fp.relative_to(SRC_DIR)\n        zf.write(fp, arcname)\n        if i % 50 == 0 or i == len(files):\n            print(f\"  Added {i}/{len(files)} -> {arcname}\")\n\nprint(\"\\nZip build complete.\")\n\n# ====== VERIFY ARCHIVE INTEGRITY ======\n# testzip() returns the name of the first corrupt file, or None if all is OK\nwith zipfile.ZipFile(OUT_ZIP, mode=\"r\") as zf:\n    bad_member = zf.testzip()\n    if bad_member is None:\n        print(\"Integrity check: OK (no corrupt members).\")\n    else:\n        print(f\"Integrity check: CORRUPT member found -> {bad_member}\")\n\n# ====== STATS & PREVIEW ======\nsize_mb = OUT_ZIP.stat().st_size / (1024 * 1024)\nprint(f\"Archive path : {OUT_ZIP}\")\nprint(f\"Archive size : {size_mb:.2f} MB\")\n\nwith zipfile.ZipFile(OUT_ZIP, mode=\"r\") as zf:\n    names = zf.namelist()\n    head = names[:10]\n    print(\"\\nPreview (first 10 entries):\")\n    for n in head:\n        print(f\"  - {n}\")\n    print(f\"... total entries: {len(names)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T03:13:58.177305Z","iopub.execute_input":"2025-08-17T03:13:58.177902Z","iopub.status.idle":"2025-08-17T03:14:13.775517Z","shell.execute_reply.started":"2025-08-17T03:13:58.177876Z","shell.execute_reply":"2025-08-17T03:14:13.774874Z"}},"outputs":[],"execution_count":null}]}